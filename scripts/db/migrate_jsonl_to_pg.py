"""
JSONLæ•°æ®è¿ç§»åˆ°PostgreSQLè„šæœ¬
å°†AI-Traderçš„JSONLæ ¼å¼æ•°æ®è¿ç§»åˆ°PostgreSQL + TimescaleDBæ•°æ®åº“
"""

import os
import sys
import json
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
import asyncpg
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.db.db_config import (
    DB_CONFIG, MIGRATION_CONFIG, get_connection_params,
    db_manager
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class JSONLToPostgreSQLMigrator:
    """
    JSONLåˆ°PostgreSQLè¿ç§»å™¨
    """

    def __init__(self):
        self.connection_params = get_connection_params()
        self.batch_size = MIGRATION_CONFIG["batch_size"]
        self.retry_attempts = MIGRATION_CONFIG["retry_attempts"]
        self.retry_delay = MIGRATION_CONFIG["retry_delay"]

    async def migrate_all(self):
        """
        æ‰§è¡Œæ‰€æœ‰è¿ç§»ä»»åŠ¡
        """
        logger.info("ğŸš€ å¼€å§‹JSONLåˆ°PostgreSQLè¿ç§»...")

        try:
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            await db_manager.initialize()

            # æ£€æŸ¥TimescaleDB
            if not await db_manager.check_timescaledb():
                logger.error("âŒ TimescaleDBæ‰©å±•æœªå®‰è£…ï¼Œæ— æ³•ç»§ç»­è¿ç§»")
                return False

            # æ‰§è¡Œè¿ç§»
            results = {}

            # 1. è¿ç§»è‚¡ç¥¨ä»·æ ¼æ•°æ®
            logger.info("ğŸ“Š è¿ç§»è‚¡ç¥¨ä»·æ ¼æ•°æ®...")
            results['stock_prices'] = await self.migrate_stock_prices()

            # 2. è¿ç§»æŒä»“å†å²æ•°æ®
            logger.info("ğŸ’¼ è¿ç§»æŒä»“å†å²æ•°æ®...")
            results['position_history'] = await self.migrate_position_history()

            # 3. è¿ç§»äº¤æ˜“æ—¥å¿—æ•°æ®
            logger.info("ğŸ“ è¿ç§»äº¤æ˜“æ—¥å¿—æ•°æ®...")
            results['trade_logs'] = await self.migrate_trade_logs()

            # 4. è¿ç§»æŒ‡æ•°æ•°æ®
            logger.info("ğŸ“ˆ è¿ç§»æŒ‡æ•°æ•°æ®...")
            results['index_prices'] = await self.migrate_index_prices()

            # ç”Ÿæˆè¿ç§»æŠ¥å‘Š
            await self.generate_migration_report(results)

            logger.info("âœ… æ‰€æœ‰è¿ç§»ä»»åŠ¡å®Œæˆ!")
            return True

        except Exception as e:
            logger.error(f"âŒ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return False
        finally:
            await db_manager.close()

    async def migrate_stock_prices(self) -> Dict[str, Any]:
        """
        è¿ç§»è‚¡ç¥¨ä»·æ ¼æ•°æ® (merged.jsonl)
        """
        results = {
            "total_files": 0,
            "total_records": 0,
            "success_records": 0,
            "error_records": 0,
            "errors": []
        }

        data_dir = Path("/e/project/AI-Trader/data/A_stock")
        jsonl_file = data_dir / "merged.jsonl"

        if not jsonl_file.exists():
            logger.warning(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_file}")
            return results

        try:
            async with db_manager._connection_pool.acquire() as conn:
                # æ¸…ç©ºç°æœ‰æ•°æ®
                await conn.execute("TRUNCATE TABLE stock_prices RESTART IDENTITY CASCADE")

                records = []
                total_lines = 0

                logger.info(f"ğŸ“– è¯»å–æ–‡ä»¶: {jsonl_file}")
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        total_lines += 1

                logger.info(f"ğŸ“Š æ€»å…± {total_lines} æ¡è®°å½•éœ€è¦è¿ç§»")

                # é‡æ–°è¯»å–å¹¶å¤„ç†æ•°æ®
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line in tqdm(f, total=total_lines, desc="è¿ç§»è‚¡ç¥¨ä»·æ ¼"):
                        try:
                            data = json.loads(line.strip())
                            record = self.parse_stock_price_record(data)
                            if record:
                                records.append(record)

                                if len(records) >= self.batch_size:
                                    await self.insert_stock_prices_batch(conn, records)
                                    results['success_records'] += len(records)
                                    records = []

                        except Exception as e:
                            results['error_records'] += 1
                            results['errors'].append(f"ç¬¬{total_lines}è¡Œ: {str(e)}")
                            logger.warning(f"è§£æå¤±è´¥: {line[:100]}... é”™è¯¯: {e}")

                # æ’å…¥å‰©ä½™è®°å½•
                if records:
                    await self.insert_stock_prices_batch(conn, records)
                    results['success_records'] += len(records)

                results['total_files'] = 1
                results['total_records'] = total_lines

                # æ›´æ–°è¿ç»­èšåˆè§†å›¾
                await self.refresh_continuous_aggregates(conn)

                logger.info(f"âœ… è‚¡ç¥¨ä»·æ ¼æ•°æ®è¿ç§»å®Œæˆ: {results['success_records']}/{total_lines} æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ è¿ç§»è‚¡ç¥¨ä»·æ ¼æ•°æ®å¤±è´¥: {e}", exc_info=True)
            results['errors'].append(str(e))

        return results

    def parse_stock_price_record(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        è§£æè‚¡ç¥¨ä»·æ ¼è®°å½•
        """
        try:
            # å¤„ç†æ—¶é—´æˆ³
            if 'timestamp' in data:
                timestamp = data['timestamp']
            elif 'date' in data:
                timestamp = f"{data['date']} 00:00:00"
            else:
                return None

            # æ ‡å‡†åŒ–å­—æ®µå
            symbol = data.get('symbol') or data.get('ts_code')
            if not symbol:
                return None

            return {
                'symbol': symbol,
                'timestamp': timestamp,
                'open_price': float(data.get('open', 0)),
                'high_price': float(data.get('high', 0)),
                'low_price': float(data.get('low', 0)),
                'close_price': float(data.get('close', 0)),
                'volume': int(data.get('volume', 0)),
                'turnover': float(data.get('turnover', 0)),
                'change_pct': float(data.get('change_pct', 0)),
                'meta_data': data
            }
        except Exception as e:
            logger.warning(f"è§£æè‚¡ç¥¨ä»·æ ¼è®°å½•å¤±è´¥: {e}")
            return None

    async def insert_stock_prices_batch(self, conn, records: List[Dict]):
        """
        æ‰¹é‡æ’å…¥è‚¡ç¥¨ä»·æ ¼æ•°æ®
        """
        query = """
            INSERT INTO stock_prices (
                symbol, timestamp, open_price, high_price, low_price,
                close_price, volume, turnover, change_pct, meta_data
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
            ON CONFLICT (symbol, timestamp) DO UPDATE SET
                open_price = EXCLUDED.open_price,
                high_price = EXCLUDED.high_price,
                low_price = EXCLUDED.low_price,
                close_price = EXCLUDED.close_price,
                volume = EXCLUDED.volume,
                turnover = EXCLUDED.turnover,
                change_pct = EXCLUDED.change_pct,
                meta_data = EXCLUDED.meta_data,
                updated_at = NOW()
        """

        values = []
        for record in records:
            values.append((
                record['symbol'],
                record['timestamp'],
                record['open_price'],
                record['high_price'],
                record['low_price'],
                record['close_price'],
                record['volume'],
                record['turnover'],
                record['change_pct'],
                json.dumps(record['meta_data'])
            ))

        await conn.executemany(query, values)

    async def migrate_position_history(self) -> Dict[str, Any]:
        """
        è¿ç§»æŒä»“å†å²æ•°æ®
        """
        results = {
            "total_files": 0,
            "total_records": 0,
            "success_records": 0,
            "error_records": 0,
            "errors": []
        }

        data_dir = Path("/e/project/AI-Trader/data/agent_data_astock")
        if not data_dir.exists():
            logger.warning(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return results

        try:
            async with db_manager._connection_pool.acquire() as conn:
                await conn.execute("TRUNCATE TABLE position_history RESTART IDENTITY CASCADE")

                # éå†æ‰€æœ‰ä»£ç†ç›®å½•
                agent_dirs = [d for d in data_dir.iterdir() if d.is_dir()]
                logger.info(f"ğŸ“ æ‰¾åˆ° {len(agent_dirs)} ä¸ªä»£ç†ç›®å½•")

                for agent_dir in tqdm(agent_dirs, desc="è¿ç§»æŒä»“æ•°æ®"):
                    agent_name = agent_dir.name
                    position_file = agent_dir / "position" / "position.jsonl"

                    if not position_file.exists():
                        continue

                    records = []
                    total_lines = 0

                    # ç»Ÿè®¡è¡Œæ•°
                    with open(position_file, 'r', encoding='utf-8') as f:
                        total_lines = sum(1 for _ in f)

                    logger.info(f"ğŸ“Š è¿ç§»ä»£ç† {agent_name}: {total_lines} æ¡è®°å½•")

                    # è¯»å–å¹¶å¤„ç†æ•°æ®
                    with open(position_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            try:
                                data = json.loads(line.strip())
                                record = self.parse_position_record(data, agent_name)
                                if record:
                                    records.append(record)

                                    if len(records) >= self.batch_size:
                                        await self.insert_positions_batch(conn, records)
                                        results['success_records'] += len(records)
                                        records = []

                            except Exception as e:
                                results['error_records'] += 1
                                results['errors'].append(f"{agent_name}: {str(e)}")

                    # æ’å…¥å‰©ä½™è®°å½•
                    if records:
                        await self.insert_positions_batch(conn, records)
                        results['success_records'] += len(records)

                    results['total_files'] += 1
                    results['total_records'] += total_lines

                logger.info(f"âœ… æŒä»“å†å²æ•°æ®è¿ç§»å®Œæˆ: {results['success_records']}/{results['total_records']} æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ è¿ç§»æŒä»“å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)
            results['errors'].append(str(e))

        return results

    def parse_position_record(self, data: Dict[str, Any], agent_name: str) -> Optional[Dict[str, Any]]:
        """
        è§£ææŒä»“è®°å½•
        """
        try:
            return {
                'agent_name': agent_name,
                'trade_date': data.get('date'),
                'trade_time': data.get('timestamp'),
                'action': data.get('this_action', {}).get('action', 'hold'),
                'symbol': data.get('this_action', {}).get('symbol'),
                'amount': int(data.get('this_action', {}).get('amount', 0)),
                'price': float(data.get('this_action', {}).get('price', 0)),
                'cash': float(data.get('cash', 0)),
                'total_value': float(data.get('total_value', 0)),
                'positions': json.dumps(data.get('positions', {})),
                'reasoning': data.get('reasoning', ''),
                'meta_data': data
            }
        except Exception as e:
            logger.warning(f"è§£ææŒä»“è®°å½•å¤±è´¥: {e}")
            return None

    async def insert_positions_batch(self, conn, records: List[Dict]):
        """
        æ‰¹é‡æ’å…¥æŒä»“æ•°æ®
        """
        query = """
            INSERT INTO position_history (
                agent_name, trade_date, trade_time, action, symbol,
                amount, price, cash, total_value, positions, reasoning, meta_data
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
        """

        values = []
        for record in records:
            values.append((
                record['agent_name'],
                record['trade_date'],
                record['trade_time'],
                record['action'],
                record['symbol'],
                record['amount'],
                record['price'],
                record['cash'],
                record['total_value'],
                record['positions'],
                record['reasoning'],
                json.dumps(record['meta_data'])
            ))

        await conn.executemany(query, values)

    async def migrate_trade_logs(self) -> Dict[str, Any]:
        """
        è¿ç§»äº¤æ˜“æ—¥å¿—æ•°æ®
        """
        results = {
            "total_files": 0,
            "total_records": 0,
            "success_records": 0,
            "error_records": 0,
            "errors": []
        }

        data_dir = Path("/e/project/AI-Trader/data/agent_data_astock")
        if not data_dir.exists():
            return results

        try:
            async with db_manager._connection_pool.acquire() as conn:
                await conn.execute("TRUNCATE TABLE trade_logs RESTART IDENTITY CASCADE")

                agent_dirs = [d for d in data_dir.iterdir() if d.is_dir()]

                for agent_dir in tqdm(agent_dirs, desc="è¿ç§»æ—¥å¿—æ•°æ®"):
                    agent_name = agent_dir.name
                    log_dir = agent_dir / "log"

                    if not log_dir.exists():
                        continue

                    # éå†æ‰€æœ‰æ—¥æœŸç›®å½•
                    for date_dir in log_dir.iterdir():
                        if not date_dir.is_dir():
                            continue

                        log_file = date_dir / "log.jsonl"
                        if not log_file.exists():
                            continue

                        records = []
                        total_lines = 0

                        # ç»Ÿè®¡è¡Œæ•°
                        with open(log_file, 'r', encoding='utf-8') as f:
                            total_lines = sum(1 for _ in f)

                        # è¯»å–å¹¶å¤„ç†æ•°æ®
                        with open(log_file, 'r', encoding='utf-8') as f:
                            for line in f:
                                try:
                                    data = json.loads(line.strip())
                                    record = self.parse_trade_log_record(data, agent_name, date_dir.name)
                                    if record:
                                        records.append(record)

                                        if len(records) >= self.batch_size:
                                            await self.insert_trade_logs_batch(conn, records)
                                            results['success_records'] += len(records)
                                            records = []

                                except Exception as e:
                                    results['error_records'] += 1
                                    results['errors'].append(f"{agent_name}/{date_dir.name}: {str(e)}")

                        # æ’å…¥å‰©ä½™è®°å½•
                        if records:
                            await self.insert_trade_logs_batch(conn, records)
                            results['success_records'] += len(records)

                        results['total_files'] += 1
                        results['total_records'] += total_lines

                logger.info(f"âœ… äº¤æ˜“æ—¥å¿—æ•°æ®è¿ç§»å®Œæˆ: {results['success_records']}/{results['total_records']} æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ è¿ç§»äº¤æ˜“æ—¥å¿—æ•°æ®å¤±è´¥: {e}", exc_info=True)
            results['errors'].append(str(e))

        return results

    def parse_trade_log_record(self, data: Dict[str, Any], agent_name: str, log_date: str) -> Optional[Dict[str, Any]]:
        """
        è§£æäº¤æ˜“æ—¥å¿—è®°å½•
        """
        try:
            return {
                'agent_name': agent_name,
                'log_timestamp': data.get('timestamp'),
                'log_date': log_date,
                'log_type': data.get('type', 'market_analysis'),
                'summary': data.get('summary', ''),
                'content': json.dumps(data),
                'tokens_used': int(data.get('tokens_used', 0)),
                'processing_time_ms': int(data.get('processing_time_ms', 0))
            }
        except Exception as e:
            logger.warning(f"è§£æäº¤æ˜“æ—¥å¿—è®°å½•å¤±è´¥: {e}")
            return None

    async def insert_trade_logs_batch(self, conn, records: List[Dict]):
        """
        æ‰¹é‡æ’å…¥äº¤æ˜“æ—¥å¿—æ•°æ®
        """
        query = """
            INSERT INTO trade_logs (
                agent_name, log_timestamp, log_date, log_type,
                summary, content, tokens_used, processing_time_ms
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
        """

        values = []
        for record in records:
            values.append((
                record['agent_name'],
                record['log_timestamp'],
                record['log_date'],
                record['log_type'],
                record['summary'],
                record['content'],
                record['tokens_used'],
                record['processing_time_ms']
            ))

        await conn.executemany(query, values)

    async def migrate_index_prices(self) -> Dict[str, Any]:
        """
        è¿ç§»æŒ‡æ•°ä»·æ ¼æ•°æ®
        """
        results = {
            "total_files": 0,
            "total_records": 0,
            "success_records": 0,
            "error_records": 0,
            "errors": []
        }

        # è¿™é‡Œå¯ä»¥æ·»åŠ æŒ‡æ•°æ•°æ®çš„è¿ç§»é€»è¾‘
        # ç›®å‰ä¸»è¦æ˜¯ä¸ºäº†æ¼”ç¤ºç»“æ„

        logger.info("â„¹ï¸  æŒ‡æ•°æ•°æ®è¿ç§»åŠŸèƒ½å¾…å®ç°")

        return results

    async def refresh_continuous_aggregates(self, conn):
        """
        åˆ·æ–°è¿ç»­èšåˆè§†å›¾
        """
        try:
            logger.info("ğŸ”„ åˆ·æ–°è¿ç»­èšåˆè§†å›¾...")

            await conn.execute("SELECT refresh_continuous_aggregate('weekly_stock_prices', NOW() - INTERVAL '1 year', NOW())")
            await conn.execute("SELECT refresh_continuous_aggregate('monthly_stock_prices', NOW() - INTERVAL '2 years', NOW())")

            logger.info("âœ… è¿ç»­èšåˆè§†å›¾åˆ·æ–°å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸  åˆ·æ–°è¿ç»­èšåˆè§†å›¾å¤±è´¥: {e}")

    async def generate_migration_report(self, results: Dict[str, Any]):
        """
        ç”Ÿæˆè¿ç§»æŠ¥å‘Š
        """
        report = {
            "migration_time": datetime.now().isoformat(),
            "summary": {
                "total_tables": len(results),
                "total_files": sum(r.get('total_files', 0) for r in results.values()),
                "total_records": sum(r.get('total_records', 0) for r in results.values()),
                "success_records": sum(r.get('success_records', 0) for r in results.values()),
                "error_records": sum(r.get('error_records', 0) for r in results.values())
            },
            "details": results
        }

        report_file = Path("migration_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“„ è¿ç§»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        logger.info(f"ğŸ“Š è¿ç§»æ‘˜è¦: {report['summary']}")

async def main():
    """
    ä¸»å‡½æ•°
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ AI-Trader JSONL åˆ° PostgreSQL è¿ç§»å·¥å…·")
    logger.info("=" * 60)

    migrator = JSONLToPostgreSQLMigrator()
    success = await migrator.migrate_all()

    if success:
        logger.info("âœ… è¿ç§»å®Œæˆ!")
        sys.exit(0)
    else:
        logger.error("âŒ è¿ç§»å¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
